{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2\n",
    "## Madusanka Madiligama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "import numpy \n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "\n",
    "def train_one_epoch(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y) # X is input (images), y is label (0-9)\n",
    "        \n",
    "        # backward pass calculates gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # take one step with these gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # resets the gradients \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "def evaluate(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - some NN pieces behave differently during training\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    loss, correct = 0, 0\n",
    "\n",
    "    # We can save computation and memory by not calculating gradients here - we aren't optimizing \n",
    "    with torch.no_grad():\n",
    "        # loop over all of the batches\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            loss += loss_fn(pred, y).item()\n",
    "            # how many are correct in this batch? Tracking for accuracy \n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    accuracy = 100*correct\n",
    "    return accuracy, loss\n",
    "\n",
    "def show_failures(model, dataloader, maxtoshow=10):\n",
    "    model.eval()\n",
    "    batch = next(iter(dataloader))\n",
    "    predictions = model(batch[0])\n",
    "    \n",
    "    rounded = predictions.argmax(1) #dimensions=1\n",
    "    errors = rounded!=batch[1] #X, y so y = label\n",
    "    print('Showing max', maxtoshow, 'first failures. '\n",
    "          'The predicted class is shown first and the correct class in parentheses.')\n",
    "    ii = 0\n",
    "    plt.figure(figsize=(maxtoshow, 1))\n",
    "    for i in range(batch[0].shape[0]):\n",
    "        if ii>=maxtoshow:\n",
    "            break\n",
    "        if errors[i]:\n",
    "            plt.subplot(1, maxtoshow, ii+1)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(batch[0][i,0,:,:], cmap=\"gray\")\n",
    "            plt.title(\"%d (%d)\" % (rounded[i], batch[1][i]))\n",
    "            ii = ii + 1\n",
    "\n",
    "class NonlinearClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "           # nn.Dropout(0.2),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "           # nn.Dropout(0.2),\n",
    "            nn.Linear(50, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.layers_stack(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First, we need to convert the input image to a vector by using \n",
    "        # nn.Flatten(). For MNIST, it means the second dimension 28*28 becomes 784.\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Here, we add a fully connected (\"dense\") layer that has 28 x 28 = 784 input nodes \n",
    "        #(one for each pixel in the input image) and 10 output nodes (for probabilities of each class).\n",
    "        self.layer_1 = nn.Linear(28*28, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.layer_1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  effect of the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Batch size = 32 ********************\n",
      "Epoch 0: training loss: 0.28353117652485765, accuracy: 91.75\n",
      "Epoch 0: val. loss: 0.2770179298122724, val. accuracy: 91.7\n",
      "Epoch 1: training loss: 0.17416470660548658, accuracy: 94.87916666666666\n",
      "Epoch 1: val. loss: 0.17754967229564986, val. accuracy: 94.69166666666666\n",
      "Epoch 2: training loss: 0.13862590005248784, accuracy: 95.91041666666666\n",
      "Epoch 2: val. loss: 0.1511280350536108, val. accuracy: 95.5\n",
      "Epoch 3: training loss: 0.1214609627610383, accuracy: 96.44375\n",
      "Epoch 3: val. loss: 0.13653348149359226, val. accuracy: 95.85833333333333\n",
      "Epoch 4: training loss: 0.10653850860536719, accuracy: 96.82916666666667\n",
      "Epoch 4: val. loss: 0.12564062214270233, val. accuracy: 96.31666666666666\n",
      "Test loss: 0.1240, test accuracy: 95.94%\n",
      "*************** Batch size = 64 ********************\n",
      "Epoch 0: training loss: 0.4805332188308239, accuracy: 87.14322916666667\n",
      "Epoch 0: val. loss: 0.48688223520914714, val. accuracy: 86.77083333333333\n",
      "Epoch 1: training loss: 0.3021263288830717, accuracy: 91.13802083333333\n",
      "Epoch 1: val. loss: 0.31471701388557755, val. accuracy: 90.63541666666667\n",
      "Epoch 2: training loss: 0.22972316240891813, accuracy: 93.19791666666667\n",
      "Epoch 2: val. loss: 0.24848241994778314, val. accuracy: 92.625\n",
      "Epoch 3: training loss: 0.18868129138213893, accuracy: 94.390625\n",
      "Epoch 3: val. loss: 0.20895197538038096, val. accuracy: 93.53125\n",
      "Epoch 4: training loss: 0.15010242975937824, accuracy: 95.59895833333333\n",
      "Epoch 4: val. loss: 0.17799517539640267, val. accuracy: 94.58333333333333\n",
      "Test loss: 0.1538, test accuracy: 95.14%\n",
      "*************** Batch size = 128 ********************\n",
      "Epoch 0: training loss: 1.9722399001320203, accuracy: 39.055989583333336\n",
      "Epoch 0: val. loss: 1.9656243026256561, val. accuracy: 39.38802083333333\n",
      "Epoch 1: training loss: 0.6143804346521695, accuracy: 81.65690104166666\n",
      "Epoch 1: val. loss: 0.6119734769066175, val. accuracy: 81.84895833333333\n",
      "Epoch 2: training loss: 0.41429517573366564, accuracy: 87.93294270833333\n",
      "Epoch 2: val. loss: 0.41561018576224645, val. accuracy: 87.91666666666667\n",
      "Epoch 3: training loss: 0.3332486108566324, accuracy: 90.62174479166667\n",
      "Epoch 3: val. loss: 0.33464240208268164, val. accuracy: 90.05208333333333\n",
      "Epoch 4: training loss: 0.28583588792632025, accuracy: 91.81315104166666\n",
      "Epoch 4: val. loss: 0.290073757370313, val. accuracy: 91.2890625\n",
      "Test loss: 0.2802, test accuracy: 91.65%\n",
      "*************** Batch size = 256 ********************\n",
      "Epoch 0: training loss: 2.2616923799117408, accuracy: 26.509602864583332\n",
      "Epoch 0: val. loss: 2.260902295509974, val. accuracy: 26.07421875\n",
      "Epoch 1: training loss: 1.9914490828911464, accuracy: 36.90185546875\n",
      "Epoch 1: val. loss: 1.9869001160065334, val. accuracy: 37.19075520833333\n",
      "Epoch 2: training loss: 1.2580999570588272, accuracy: 59.659830729166664\n",
      "Epoch 2: val. loss: 1.2493347922960918, val. accuracy: 60.725911458333336\n",
      "Epoch 3: training loss: 0.8058115274955829, accuracy: 76.21256510416666\n",
      "Epoch 3: val. loss: 0.8018820732831955, val. accuracy: 76.39973958333334\n",
      "Epoch 4: training loss: 0.6005760813131928, accuracy: 81.94173177083334\n",
      "Epoch 4: val. loss: 0.5996986404061317, val. accuracy: 81.689453125\n",
      "Test loss: 0.5843, test accuracy: 82.22%\n",
      "*************** Batch size = 512 ********************\n",
      "Epoch 0: training loss: 2.28953287540338, accuracy: 15.386571719226858\n",
      "Epoch 0: val. loss: 2.2893401861190794, val. accuracy: 15.622457282343369\n",
      "Epoch 1: training loss: 2.2650605592972193, accuracy: 24.201424211597153\n",
      "Epoch 1: val. loss: 2.2651286840438845, val. accuracy: 24.430431244914562\n",
      "Epoch 2: training loss: 2.215953380633623, accuracy: 29.109867751780268\n",
      "Epoch 2: val. loss: 2.2164131879806517, val. accuracy: 30.370219690805534\n",
      "Epoch 3: training loss: 2.090596052316519, accuracy: 43.71820956256358\n",
      "Epoch 3: val. loss: 2.091970753669739, val. accuracy: 45.097640358014644\n",
      "Epoch 4: training loss: 1.7812000451943812, accuracy: 56.99389623601221\n",
      "Epoch 4: val. loss: 1.7859917998313903, val. accuracy: 57.66883645240033\n",
      "Test loss: 1.7699, test accuracy: 58.00%\n",
      "CPU times: user 57min 16s, sys: 869 ms, total: 57min 17s\n",
      "Wall time: 2min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_sizes = [32, 64, 128, 256, 512]\n",
    "    # batch_size = 128\n",
    "for batch_size in batch_sizes: \n",
    "\n",
    "    train_size = int(0.8 * len(training_data))  # 80% for training\n",
    "    val_size = len(training_data) - train_size  # Remaining 20% for validation\n",
    "    training_data, validation_data = torch.utils.data.random_split(training_data, [train_size, val_size], generator=torch.Generator().manual_seed(55))\n",
    "    \n",
    "\n",
    "    print(f'*************** Batch size = {batch_size} ********************') \n",
    "    nonlinear_model = NonlinearClassifier()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(nonlinear_model.parameters(), lr=0.05)\n",
    "    \n",
    "    # The dataloader makes our dataset iterable \n",
    "    train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\n",
    "    val_dataloader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)\n",
    "    \n",
    "    \n",
    "    \n",
    "    epochs = 5\n",
    "    train_acc_all = []\n",
    "    val_acc_all = []\n",
    "    for j in range(epochs):\n",
    "        train_one_epoch(train_dataloader, nonlinear_model, loss_fn, optimizer)\n",
    "        \n",
    "        # checking on the training loss and accuracy once per epoch\n",
    "        acc, loss = evaluate(train_dataloader, nonlinear_model, loss_fn)\n",
    "        train_acc_all.append(acc)\n",
    "        print(f\"Epoch {j}: training loss: {loss}, accuracy: {acc}\")\n",
    "        \n",
    "        # checking on the validation loss and accuracy once per epoch\n",
    "        val_acc, val_loss = evaluate(val_dataloader, nonlinear_model, loss_fn)\n",
    "        val_acc_all.append(val_acc)\n",
    "        print(f\"Epoch {j}: val. loss: {val_loss}, val. accuracy: {val_acc}\")\n",
    "\n",
    "    #finally, evaluate how it performs against the test data: \n",
    "    batch_size_test = 256\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size_test)\n",
    "    acc_test, loss_test = evaluate(test_dataloader, nonlinear_model, loss_fn)\n",
    "    print(\"Test loss: %.4f, test accuracy: %.2f%%\" % (loss_test, acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between batch size and test accuracy is indeed significant. Smaller batch sizes tend to yield higher final test accuracies, as seen in various trials where smaller batches like 32 achieved test accuracies above 95%, while larger batches like 512 often resulted in lower accuracies around 58%. However, training with smaller batch sizes increases the computational burden, leading to longer training times due to the higher number of updates required during the training process. To balance between accuracy and training efficiency, a moderate batch size in the range of 64 to 128 is often recommended. This range allows for effective learning dynamics while ensuring that the computational resource demands remain manageable. Adopting this strategy facilitates achieving high test accuracy without disproportionately extending training durations or complicating resource management. Ultimately, selecting an optimal batch size is not just about accuracy alone but also about finding a practical approach that suits the dataset and model being trained. This nuanced approach can lead to more robust and efficient training outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## effect of the learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Learning Rate = 0.01 ********************\n",
      "Epoch 0: training loss: 2.2981832356956917, accuracy: 9.893184130213632\n",
      "Epoch 0: val. loss: 2.2991538355427403, val. accuracy: 9.638860630722279\n",
      "Epoch 1: training loss: 2.286097858010269, accuracy: 10.147507629704986\n",
      "Epoch 1: val. loss: 2.287155036003359, val. accuracy: 9.893184130213632\n",
      "Epoch 2: training loss: 2.2684968525801246, accuracy: 17.427517802644964\n",
      "Epoch 2: val. loss: 2.2698019012328117, val. accuracy: 17.16683621566633\n",
      "Epoch 3: training loss: 2.237386178194992, accuracy: 20.879959308240082\n",
      "Epoch 3: val. loss: 2.23907962922127, val. accuracy: 21.337741607324517\n",
      "Epoch 4: training loss: 2.179336290049359, accuracy: 29.65412004069176\n",
      "Epoch 4: val. loss: 2.181519023833736, val. accuracy: 29.120040691759918\n",
      "Test loss: 2.1772, test accuracy: 30.72%\n",
      "*************** Learning Rate = 0.05 ********************\n",
      "Epoch 0: training loss: 2.2770631313323975, accuracy: 19.106660308377048\n",
      "Epoch 0: val. loss: 2.2781411266326903, val. accuracy: 18.467895740623014\n",
      "Epoch 1: training loss: 2.0942207225645433, accuracy: 28.302336671435384\n",
      "Epoch 1: val. loss: 2.10026086807251, val. accuracy: 28.607755880483154\n",
      "Epoch 2: training loss: 1.3585471540990501, accuracy: 55.36480686695279\n",
      "Epoch 2: val. loss: 1.3664646339416504, val. accuracy: 55.49904640813732\n",
      "Epoch 3: training loss: 0.802641440521587, accuracy: 73.66078524876808\n",
      "Epoch 3: val. loss: 0.8014473438262939, val. accuracy: 74.50731087094724\n",
      "Epoch 4: training loss: 0.5822324379526004, accuracy: 82.26831982196789\n",
      "Epoch 4: val. loss: 0.5897137069702149, val. accuracy: 82.453909726637\n",
      "Test loss: 0.5689, test accuracy: 82.81%\n",
      "*************** Learning Rate = 0.1 ********************\n",
      "Epoch 0: training loss: 2.238627125945272, accuracy: 20.486835568802782\n",
      "Epoch 0: val. loss: 2.238579571247101, val. accuracy: 20.103297576479935\n",
      "Epoch 1: training loss: 1.2780678347696233, accuracy: 56.40337804272231\n",
      "Epoch 1: val. loss: 1.2693359494209289, val. accuracy: 55.423122765196666\n",
      "Epoch 2: training loss: 0.6266768435134163, accuracy: 79.78142076502732\n",
      "Epoch 2: val. loss: 0.6278388753533364, val. accuracy: 79.4994040524434\n",
      "Epoch 3: training loss: 0.5065082390851612, accuracy: 83.60655737704919\n",
      "Epoch 3: val. loss: 0.5195491716265679, val. accuracy: 82.63806118394913\n",
      "Epoch 4: training loss: 0.41472152705434, accuracy: 87.19324391455538\n",
      "Epoch 4: val. loss: 0.4379438400268555, val. accuracy: 86.21374652363926\n",
      "Test loss: 0.4169, test accuracy: 87.09%\n",
      "*************** Learning Rate = 0.2 ********************\n",
      "Epoch 0: training loss: 1.7869249487680101, accuracy: 42.36214605067064\n",
      "Epoch 0: val. loss: 1.812236301600933, val. accuracy: 40.78489816194734\n",
      "Epoch 1: training loss: 0.9681391725464473, accuracy: 62.74217585692996\n",
      "Epoch 1: val. loss: 1.009216383099556, val. accuracy: 62.4441132637854\n",
      "Epoch 2: training loss: 0.4235963551771073, accuracy: 87.59314456035767\n",
      "Epoch 2: val. loss: 0.46966007351875305, val. accuracy: 86.48782911077994\n",
      "Epoch 3: training loss: 0.3554415274707098, accuracy: 89.23248882265275\n",
      "Epoch 3: val. loss: 0.4279826432466507, val. accuracy: 86.9846000993542\n",
      "Epoch 4: training loss: 0.2708107769962341, accuracy: 92.03924490809736\n",
      "Epoch 4: val. loss: 0.35444228630512953, val. accuracy: 89.41877794336811\n",
      "Test loss: 0.3095, test accuracy: 90.36%\n",
      "CPU times: user 14min 54s, sys: 371 ms, total: 14min 55s\n",
      "Wall time: 39.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.2]\n",
    "    # batch_size = 128\n",
    "for lr_ in learning_rates: \n",
    "    \n",
    "    train_size = int(0.8 * len(training_data))  # 80% for training\n",
    "    val_size = len(training_data) - train_size  # Remaining 20% for validation\n",
    "    # print(f'train_size: {train_size}')\n",
    "    # print(f'val_size: {val_size}')\n",
    "    training_data, validation_data = torch.utils.data.random_split(training_data, [train_size, val_size], generator=torch.Generator().manual_seed(55))\n",
    "    \n",
    "    batch_size = 128 #keep constant\n",
    "    print(f'*************** Learning Rate = {lr_} ********************') \n",
    "    nonlinear_model = NonlinearClassifier()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(nonlinear_model.parameters(), lr=lr_)\n",
    "    \n",
    "    # The dataloader makes our dataset iterable \n",
    "    train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\n",
    "    val_dataloader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)\n",
    "    \n",
    "    \n",
    "    \n",
    "    epochs = 5\n",
    "    train_acc_all = []\n",
    "    val_acc_all = []\n",
    "    for j in range(epochs):\n",
    "        train_one_epoch(train_dataloader, nonlinear_model, loss_fn, optimizer)\n",
    "        \n",
    "        # checking on the training loss and accuracy once per epoch\n",
    "        acc, loss = evaluate(train_dataloader, nonlinear_model, loss_fn)\n",
    "        train_acc_all.append(acc)\n",
    "        print(f\"Epoch {j}: training loss: {loss}, accuracy: {acc}\")\n",
    "        \n",
    "        # checking on the validation loss and accuracy once per epoch\n",
    "        val_acc, val_loss = evaluate(val_dataloader, nonlinear_model, loss_fn)\n",
    "        val_acc_all.append(val_acc)\n",
    "        print(f\"Epoch {j}: val. loss: {val_loss}, val. accuracy: {val_acc}\")\n",
    "\n",
    "    #finally, evaluate how it performs against the test data: \n",
    "    batch_size_test = 256\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size_test)\n",
    "    acc_test, loss_test = evaluate(test_dataloader, nonlinear_model, loss_fn)\n",
    "    print(\"Test loss: %.4f, test accuracy: %.2f%%\" % (loss_test, acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "​The learning rate is a critical hyperparameter influencing the performance and quality of a machine learning model.​ As observed, a learning rate of 0.05 resulted in a notable improvement in model accuracy, achieving a final test accuracy of 82.81%, showcasing significant learning progress throughout the epochs. In contrast, a lower learning rate of 0.01 led to very modest accuracies, peaking at just 30.72%, indicating insufficient weight updates for effective learning. In comparison, an excessively high learning rate of 0.2 offered a substantial lift to 90.36% accuracy but posed risks of divergence during training.\n",
    "\n",
    "It appears that moderate learning rates foster stable convergence better than the extremes. A learning rate of 0.1 also produced a solid performance, reaching an accuracy of 87.09%, suggesting that it effectively balances the need for rapid training updates with maintaining stable convergence. Ideally, a low learning rate hinders learning, while too high risks overshooting optimal weights, resulting in convergence challenges. Thus, selecting an optimal learning rate is paramount for achieving high model accuracy efficiently and effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  effect of the activation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Nonlinear Model ********************\n",
      "Epoch 0: training loss: 2.294995850207759, accuracy: 13.538270454898308\n",
      "Epoch 0: val. loss: 2.294008365044227, val. accuracy: 14.400993171942892\n",
      "Epoch 1: training loss: 2.279382822560329, accuracy: 21.471821145784816\n",
      "Epoch 1: val. loss: 2.2795231892512393, val. accuracy: 20.235878336436997\n",
      "Epoch 2: training loss: 2.240607504751168, accuracy: 28.27200745225897\n",
      "Epoch 2: val. loss: 2.241924817745502, val. accuracy: 27.063935443823713\n",
      "Epoch 3: training loss: 2.107277337242575, accuracy: 43.067846607669615\n",
      "Epoch 3: val. loss: 2.1121602058410645, val. accuracy: 40.285536933581625\n",
      "Epoch 4: training loss: 1.7365573364145614, accuracy: 44.65145163794442\n",
      "Epoch 4: val. loss: 1.7549063517497137, val. accuracy: 42.70639354438237\n",
      "Test loss: 1.7300, test accuracy: 44.28%\n",
      "*************** Linear Model ********************\n",
      "Epoch 0: training loss: 2.3350457621783747, accuracy: 7.686335403726709\n",
      "Epoch 0: val. loss: 2.3404565074227075, val. accuracy: 8.068269976726144\n",
      "Epoch 1: training loss: 2.3350457621783747, accuracy: 7.686335403726709\n",
      "Epoch 1: val. loss: 2.3404565074227075, val. accuracy: 8.068269976726144\n",
      "Epoch 2: training loss: 2.3350457621783747, accuracy: 7.686335403726709\n",
      "Epoch 2: val. loss: 2.3404565074227075, val. accuracy: 8.068269976726144\n",
      "Epoch 3: training loss: 2.3350457621783747, accuracy: 7.686335403726709\n",
      "Epoch 3: val. loss: 2.3404565074227075, val. accuracy: 8.068269976726144\n",
      "Epoch 4: training loss: 2.3350457621783747, accuracy: 7.686335403726709\n",
      "Epoch 4: val. loss: 2.3404565074227075, val. accuracy: 8.068269976726144\n",
      "Test loss: 2.3386, test accuracy: 7.74%\n",
      "CPU times: user 3min 47s, sys: 263 ms, total: 3min 47s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "########### Nonlinear Model\n",
    "\n",
    "train_size = int(0.8 * len(training_data))  # 80% for training\n",
    "val_size = len(training_data) - train_size  # Remaining 20% for validation\n",
    "# print(f'train_size: {train_size}')\n",
    "# print(f'val_size: {val_size}')\n",
    "training_data, validation_data = torch.utils.data.random_split(training_data, [train_size, val_size], generator=torch.Generator().manual_seed(55))\n",
    "\n",
    "batch_size = 128 #keep constant\n",
    "print(f'*************** Nonlinear Model ********************') \n",
    "nonlinear_model = NonlinearClassifier()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(nonlinear_model.parameters(), lr=0.05)\n",
    "\n",
    "# The dataloader makes our dataset iterable \n",
    "train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\n",
    "val_dataloader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "train_acc_all = []\n",
    "val_acc_all = []\n",
    "for j in range(epochs):\n",
    "    train_one_epoch(train_dataloader, nonlinear_model, loss_fn, optimizer)\n",
    "    \n",
    "    # checking on the training loss and accuracy once per epoch\n",
    "    acc, loss = evaluate(train_dataloader, nonlinear_model, loss_fn)\n",
    "    train_acc_all.append(acc)\n",
    "    print(f\"Epoch {j}: training loss: {loss}, accuracy: {acc}\")\n",
    "    \n",
    "    # checking on the validation loss and accuracy once per epoch\n",
    "    val_acc, val_loss = evaluate(val_dataloader, nonlinear_model, loss_fn)\n",
    "    val_acc_all.append(val_acc)\n",
    "    print(f\"Epoch {j}: val. loss: {val_loss}, val. accuracy: {val_acc}\")\n",
    "\n",
    "#finally, evaluate how it performs against the test data: \n",
    "batch_size_test = 256\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size_test)\n",
    "acc_test, loss_test = evaluate(test_dataloader, nonlinear_model, loss_fn)\n",
    "print(\"Test loss: %.4f, test accuracy: %.2f%%\" % (loss_test, acc_test))\n",
    "\n",
    "############ Linear Model\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(training_data))  # 80% for training\n",
    "val_size = len(training_data) - train_size  # Remaining 20% for validation\n",
    "# print(f'train_size: {train_size}')\n",
    "# print(f'val_size: {val_size}')\n",
    "training_data, validation_data = torch.utils.data.random_split(training_data, [train_size, val_size], generator=torch.Generator().manual_seed(55))\n",
    "\n",
    "batch_size = 128 #keep constant\n",
    "print(f'*************** Linear Model ********************') \n",
    "linear_model = LinearClassifier()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(nonlinear_model.parameters(), lr=0.05)\n",
    "\n",
    "# The dataloader makes our dataset iterable \n",
    "train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\n",
    "val_dataloader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)\n",
    "\n",
    "epochs = 5\n",
    "train_acc_all = []\n",
    "val_acc_all = []\n",
    "for j in range(epochs):\n",
    "    train_one_epoch(train_dataloader, linear_model, loss_fn, optimizer)\n",
    "    \n",
    "    # checking on the training loss and accuracy once per epoch\n",
    "    acc, loss = evaluate(train_dataloader, linear_model, loss_fn)\n",
    "    train_acc_all.append(acc)\n",
    "    print(f\"Epoch {j}: training loss: {loss}, accuracy: {acc}\")\n",
    "    \n",
    "    # checking on the validation loss and accuracy once per epoch\n",
    "    val_acc, val_loss = evaluate(val_dataloader, linear_model, loss_fn)\n",
    "    val_acc_all.append(val_acc)\n",
    "    print(f\"Epoch {j}: val. loss: {val_loss}, val. accuracy: {val_acc}\")\n",
    "    \n",
    "#finally, evaluate how it performs against the test data: \n",
    "batch_size_test = 256\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size_test)\n",
    "acc_test, loss_test = evaluate(test_dataloader, linear_model, loss_fn)\n",
    "print(\"Test loss: %.4f, test accuracy: %.2f%%\" % (loss_test, acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "​The choice of activation function significantly impacts a model's performance and quality, as demonstrated by the contrasting results between the nonlinear and linear models.​ The nonlinear model exhibited a gradual improvement in training accuracy, reaching 44.65% over five epochs, which indicates its ability to learn complex patterns due to the introduction of non-linearity. In contrast, the linear model struggled to move beyond a mere 7.69% accuracy, demonstrating stagnation across all epochs, a clear indication that the linear activation function failed to capture the underlying complexities in the data.\n",
    "\n",
    "Activation functions like ReLU or sigmoid in nonlinear models allow for better adaptability during training by facilitating the model to approximate a wider variety of functions. This flexibility enables the network to learn from more intricate relationships within the data and enhances overall performance metrics such as accuracy and loss. On the other hand, employing a linear activation function restricts the model's capacity to learn beyond linear relationships, often resulting in poor convergence and limited accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: What is a learning rate scheduler?\n",
    "​A learning rate scheduler is a machine learning technique to adjust an optimizer's learning rate during the training process.​ Instead of using a constant learning rate throughout training, a scheduler dynamically changes the learning rate based on specific criteria, such as the epoch number or training performance. This can help improve model convergence by allowing for larger updates at the beginning of training (to speed up learning) and smaller updates later (to fine-tune the model). Common types of learning rate schedules include constant schedules, exponential decay, step decay, and cyclical learning rates.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda/2024-04-29",
   "language": "python",
   "name": "2024-04-29"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
